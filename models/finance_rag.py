import os
import json
import asyncio
from typing import List, TypedDict
from tqdm import tqdm
from dotenv import load_dotenv

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from langgraph.graph import StateGraph, END

# 1. ìƒíƒœ(State) ì •ì˜: ë…¸ë“œ ê°„ì— ì „ë‹¬ë  ë°ì´í„° êµ¬ì¡°
class AgentState(TypedDict):
    question: str
    context: List[Document]
    answer: str
    retry_count: int
    relevance: str  # <--- ì´ ì¤„ì´ ë°˜ë“œì‹œ ìˆì–´ì•¼ í•©ë‹ˆë‹¤!

class FinanceRAG:
    def __init__(self, db_dir="./finance_local_db"):
        load_dotenv()
        self.db_dir = db_dir
        self.embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask", model_kwargs={'device': 'cpu'})
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite", temperature=0)
        
        # ë²¡í„° DB ë¡œë“œ
        self.vector_db = Chroma(persist_directory=self.db_dir, embedding_function=self.embeddings)
        
        # 2. ê·¸ë˜í”„ êµ¬ì¶•
        self.app = self._build_graph()

    def _build_graph(self):
        workflow = StateGraph(AgentState)

        # 1. ë…¸ë“œ ì •ì˜: ê° ë‹¨ê³„ì˜ ì—­í•  ì§€ì •
        workflow.add_node("retrieve", self.node_retrieve)               # RAG: ì§ˆë¬¸ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        workflow.add_node("grade_documents", self.node_grade_documents) # QC: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì í•©ì„± í‰ê°€
        workflow.add_node("generate", self.node_generate)               # ìµœì¢… ë‹µë³€ ìƒì„±

        # 2. ê¸°ë³¸ ì—£ì§€: ê²€ìƒ‰ì´ ëë‚˜ë©´ ë¬´ì¡°ê±´ í‰ê°€ ë‹¨ê³„ë¡œ ì´ë™
        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "grade_documents")
        
        # 3. ì¡°ê±´ë¶€ ì—£ì§€: í‰ê°€ ê²°ê³¼(relevance)ì— ë”°ë¥¸ ë¶„ê¸° ì²˜ë¦¬
        workflow.add_conditional_edges(
            "grade_documents",
            self.decide_to_generate,
            {
                "generate": "generate", # ì í•©: ë‹µë³€ ìƒì„±ìœ¼ë¡œ ì´ë™
                "rewrite": "retrieve",  # ë¶€ì í•©: ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì¡°ì • í›„ ë‹¤ì‹œ ê²€ìƒ‰
                "end": END              # ì‹¤íŒ¨: ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ì¢…ë£Œ
            }
        )
        
        # 4. ì¢…ë£Œ ì—£ì§€: ë‹µë³€ ìƒì„±ì´ ì™„ë£Œë˜ë©´ ë
        workflow.add_edge("generate", END)

        return workflow.compile()

    # --- [ë…¸ë“œ í•¨ìˆ˜ë“¤] ---

    def node_retrieve(self, state: AgentState):
        print("ğŸ” [Node: Retrieve] ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ëŠ” ì¤‘...")
        question = state["question"]
        # k=5ë¡œ ê²€ìƒ‰
        docs = self.vector_db.as_retriever(search_kwargs={"k": 5}).invoke(question)
        return {"context": docs, "retry_count": state.get("retry_count", 0) + 1}
    
    # === [ langgraph í†µê³¼ í•¨ìˆ˜ ]
    # def node_grade_documents(self, state: AgentState):
    #     print("âš–ï¸ [Node: Grade] (ì„ì‹œ) API í˜¸ì¶œ ì—†ì´ í†µê³¼ ëª¨ë“œ...")
        
    #     # LLM í˜¸ì¶œ ì½”ë“œë¥¼ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ë¬´ì‹œí•˜ê³ 
    #     # ë¬´ì¡°ê±´ 'yes'ë¥¼ ë°˜í™˜í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.
    #     return {"relevance": "yes"}

    def node_grade_documents(self, state: AgentState):
        print("âš–ï¸ [Node: Grade] ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘...")
        question = state["question"]
        docs = state["context"]

        if not docs:
            print("âŒ [Grade] ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì•„ì˜ˆ ì—†ìŒ")
            return {"relevance": "no"}

        # 1. LLMì—ê²Œ íŒë‹¨ ìš”ì²­ (ë” ì§ê´€ì ì¸ í”„ë¡¬í”„íŠ¸)
        prompt = ChatPromptTemplate.from_template("""
        ë„ˆëŠ” ë°ì´í„° ë¶„ì„ê°€ì•¼. ì•„ë˜ [ë¬¸ì„œ]ì— [ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µì„ í•  ìˆ˜ ìˆëŠ” ìˆ«ìê°€ í•˜ë‚˜ë¼ë„ ë“¤ì–´ìˆë‹ˆ?

        ì˜ˆë¥¼ ë“¤ì–´ ì§ˆë¬¸ì´ 'ì‚¼ì„±ì „ì 2025ë…„ ì˜ì—…ì´ìµ'ì´ê³ , 
        ë¬¸ì„œì— 'ì‚¼ì„±ì „ì', '2025', 'ì˜ì—…ì´ìµ'ì´ë¼ëŠ” ê¸€ìì™€ ìˆ«ìê°€ ìˆë‹¤ë©´ ë¬´ì¡°ê±´ 'yes'ë¼ê³  í•´.

        ë‹µë³€ì€ ë”± í•œ ë‹¨ì–´ 'yes' ë˜ëŠ” 'no'ë¡œë§Œ í•´.

        [ì§ˆë¬¸]: {question}
        [ë¬¸ì„œ]: {docs}

        ê²°ì •:
        """)
    
        chain = prompt | self.llm | StrOutputParser()
        # LLMì˜ ì‹¤ì œ ë‹µë³€ì„ raw_resultì— ë‹´ì•„ ì¶œë ¥í•´ë´…ë‹ˆë‹¤.
        raw_result = chain.invoke({"question": question, "docs": docs}).lower().strip()

        print(f"ğŸ¤– [Grade] LLMì˜ ì‹¤ì œ íŒë‹¨: '{raw_result}'")

        # 2. ê²°ê³¼ íŒì • (ì•ˆì „ì¥ì¹˜ ì¶”ê°€: yesê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, íŠ¹ì • í‚¤ì›Œë“œ ë§¤ì¹­ ì‹œ í†µê³¼)
        if "yes" in raw_result:
            print("âœ… [Grade] ê²°ê³¼: YES")
            return {"relevance": "yes"} # <--- í‚¤ ì´ë¦„ì´ AgentStateì™€ ê°™ì•„ì•¼ í•¨
        else:
            print("âŒ [Grade] ê²°ê³¼: NO")
            return {"relevance": "no"}

    def decide_to_generate(self, state: AgentState):
        # state["relevance"]ë¥¼ ì§ì ‘ ì ‘ê·¼í•´ì„œ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
        relevance = state.get("relevance")
        retry_count = state.get("retry_count", 0)
        
        # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        print(f"ğŸ§ [Decision Debug] í˜„ì¬ ìƒíƒœì˜ relevance: '{relevance}'")
    
        if relevance == "yes":
            print("âœ¨ [Decision] í†µê³¼! ìƒì„± ë…¸ë“œë¡œ ì´ë™")
            return "generate"
        
        if retry_count > 2:
            return "end"
        
        return "rewrite"

    def node_generate(self, state: AgentState):
        print("âœï¸ [Node: Generate] ë‹µë³€ ìƒì„± ì¤‘...")
        docs = state["context"]
        question = state["question"]
        
        if not docs: return {"relevance": "no"}
    
        # 1ì°¨ í•„í„°ë§: ì§ˆë¬¸ì˜ í•µì‹¬ ë‹¨ì–´ê°€ ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ LLM í˜¸ì¶œ ì—†ì´ í†µê³¼!
        keywords = [question[:4], "ì‚¼ì„±", "ì „ì"] # ì˜ˆì‹œ í‚¤ì›Œë“œ
        if any(k in docs[0].page_content for k in keywords):
            print("âš¡ [Grade] í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ API í˜¸ì¶œ ì—†ì´ í†µê³¼!")
            return {"relevance": "yes"}
        context = "\n\n".join([d.page_content for d in state["context"]])
        
        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
        ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

        [ë°ì´í„°]
        {context}

        ì§ˆë¬¸: {question}
        """)
        
        chain = prompt | self.llm | StrOutputParser()
        answer = chain.invoke({"context": context, "question": question})
        return {"answer": answer}

    # --- [ì™¸ë¶€ í˜¸ì¶œ ë©”ì„œë“œ] ---

    async def query_stream(self, question: str):
        inputs = {"question": question, "retry_count": 0}
        
        # 1. ê·¸ë˜í”„ ì‹¤í–‰
        final_state = await asyncio.to_thread(self.app.invoke, inputs)
        
        # 2. íŒë‹¨ ê²°ê³¼ í™•ì¸
        if final_state.get("relevance") != "yes":
            yield "âŒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •í™•í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë°ì´í„° ë¶€ì¡±)"
            return

        # 3. 'yes'ì¼ ë•Œë§Œ Gemini ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘
        context = "\n\n".join([d.page_content for d in final_state["context"]])
        prompt = f"ì•„ë˜ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n{context}\n\nì§ˆë¬¸: {question}"
        
        async for chunk in self.llm.astream(prompt):
            yield chunk.content