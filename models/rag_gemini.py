# ragë¥¼ ì ìš©í•œ llm
import os
import json
import time
from tqdm import tqdm
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.documents import Document

# api ë©”ì„œë“œ
import asyncio

class FinanceRAG:
    def __init__(self, db_dir="./finance_local_db"):
        load_dotenv()
        self.db_dir = db_dir
        
        # 1. ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ (4500U CPUì—ì„œ ì‘ë™)
        print("ğŸ’¡ ë¡œì»¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'}
        )
        
        # 2. ë‹µë³€ìš© LLM
        self.llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
        self.vector_db = None

        # ê¸°ì¡´ DB ë¡œë“œ
        if os.path.exists(self.db_dir) and os.path.isdir(self.db_dir) and os.listdir(self.db_dir):
            print(f"ğŸ“¦ ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ: {self.db_dir}")
            self.vector_db = Chroma(
                persist_directory=self.db_dir, 
                embedding_function=self.embeddings
            )
        else:
            print("â„¹ï¸ ê¸°ì¡´ DBê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ êµ¬ì¶•ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    def _parse_jsonl(self, file_path):
        """íŒŒì¼ì„ ì½ì–´ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ê³µí†µ ë¡œì§"""
        documents = []
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for line in tqdm(lines, desc="ë°ì´í„° íŒŒì‹± ì¤‘"):
                try:
                    item = json.loads(line)
                    content_json = json.loads(item['output'])
                    meta = content_json['metadata']
                    
                    text_content = f"ê¸°ì—…ëª…: {meta['company']}, ì—°ë„: {meta['fiscal_year']}\n"
                    text_content += f"ì¬ë¬´: {content_json['financial_metrics']}\n"
                    text_content += f"ë¹„ìœ¨: {content_json['analysis_ratios']}"
                    
                    documents.append(Document(
                        page_content=text_content, 
                        metadata={"company": str(meta['company']), "year": str(meta['fiscal_year'])}
                    ))
                except: continue
        return documents

    def ingest_local_json(self, file_path):
        """ì²˜ìŒë¶€í„° DBë¥¼ ìƒì„± (ì „ì²´ 6,000ì¤„ìš©)"""
        docs = self._parse_jsonl(file_path)
        print(f"ğŸš€ {len(docs)}ê±´ ë¡œì»¬ ì„ë² ë”© ì‹œì‘... (4500U ê¸°ì¤€ ì•½ 5-10ë¶„)")
        self.vector_db = Chroma.from_documents(
            documents=docs,
            embedding=self.embeddings,
            persist_directory=self.db_dir
        )
        print("âœ… DB êµ¬ì¶• ì™„ë£Œ!")

    def update_data(self, file_path):
        """ê¸°ì¡´ DBì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€"""
        if self.vector_db is None:
            self.ingest_local_json(file_path)
            return
            
        docs = self._parse_jsonl(file_path)
        print(f"ğŸš€ {len(docs)}ê±´ì˜ ë°ì´í„°ë¥¼ ì¶”ê°€ ì¤‘...")
        self.vector_db.add_documents(docs)
        print("âœ… ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")

    def query(self, question):
        if not self.vector_db:
            return "âŒ DBê°€ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì„ ë¨¼ì € ì§„í–‰í•˜ì„¸ìš”."
        
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
        docs = retriever.invoke(question)
        context = "\n\n".join([d.page_content for d in docs])
        
        prompt = f"ì•„ë˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n[ë°ì´í„°]\n{context}\n\nì§ˆë¬¸: {question}"
        return self.llm.invoke(prompt).content

    async def query_stream(self, question: str):
        if not self.vector_db:
            yield "âŒ DBê°€ ì—†ìŠµë‹ˆë‹¤."
            return

        # 1. ê²€ìƒ‰ (ì´ ë‹¨ê³„ëŠ” ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹ˆë¯€ë¡œ ë¹ ë¥´ê²Œ ìˆ˜í–‰)
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
        docs = await asyncio.to_thread(retriever.invoke, question) # ë¹„ë™ê¸° ì²˜ë¦¬
        context = "\n\n".join([d.page_content for d in docs])

        prompt = f"ì•„ë˜ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n[ë°ì´í„°]\n{context}\n\nì§ˆë¬¸: {question}"

        # 2. ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë° (Geminiê°€ í•œ ê¸€ìì”© ë³´ëƒ„)
        async for chunk in self.llm.astream(prompt):
            yield chunk.content # í•œ í† í°ì”© í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ë‹¬